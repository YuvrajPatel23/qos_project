{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d0e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data cleaned.\n",
      "traffic_class\n",
      "Non-Streaming    5183\n",
      "Streaming         802\n",
      "Name: count, dtype: int64\n",
      "     category  traffic_class\n",
      "0  FILE-SKYPE  Non-Streaming\n",
      "1  FILE-SKYPE  Non-Streaming\n",
      "2  FILE-SKYPE  Non-Streaming\n",
      "3  FILE-SKYPE  Non-Streaming\n",
      "4  FILE-SKYPE  Non-Streaming\n",
      "\n",
      "âœ… Feature columns extracted: 40 features\n",
      "Features: ['forward_pl_mean', 'forward_piat_mean', 'forward_pps_mean', 'forward_bps_mean', 'forward_pl_var']...\n",
      "\n",
      "âœ… Label encoding completed.\n",
      "Classes: ['Non-Streaming' 'Streaming']\n",
      "Encoded values: {'Non-Streaming': 0, 'Streaming': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuvraj\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SMOTE balancing completed.\n",
      "Original class distribution: [5183  802]\n",
      "Balanced class distribution: [5183 5183]\n",
      "\n",
      "âœ… Train/test split completed.\n",
      "Training samples: 8292\n",
      "Testing samples: 2074\n",
      "\n",
      "ðŸš€ Training Random Forest model...\n",
      "\n",
      "âœ… Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Streaming       0.99      0.99      0.99      1037\n",
      "    Streaming       0.99      0.99      0.99      1037\n",
      "\n",
      "     accuracy                           0.99      2074\n",
      "    macro avg       0.99      0.99      0.99      2074\n",
      " weighted avg       0.99      0.99      0.99      2074\n",
      "\n",
      "\n",
      "ðŸ’¾ Saving models for QoS pipeline integration...\n",
      "âœ… Models saved successfully:\n",
      "  - traffic_classifier.pkl\n",
      "  - label_encoder.pkl\n",
      "  - feature_columns.pkl\n",
      "\n",
      "ðŸ“Š Model Summary:\n",
      "Model type: RandomForestClassifier\n",
      "Number of estimators: 200\n",
      "Number of features: 40\n",
      "Number of classes: 2\n",
      "Classes: ['Non-Streaming', 'Streaming']\n",
      "\n",
      "ðŸŽ¯ Top 10 Most Important Features:\n",
      "         feature  importance\n",
      "  reverse_pl_max    0.136294\n",
      "  forward_pl_min    0.094914\n",
      " reverse_pl_mean    0.071970\n",
      "   reverse_pl_q3    0.062580\n",
      "  reverse_pl_min    0.057888\n",
      "   forward_pl_q3    0.049328\n",
      " forward_pl_mean    0.049148\n",
      "   forward_pl_q1    0.041712\n",
      " reverse_bps_max    0.037903\n",
      "forward_bps_mean    0.037678\n",
      "\n",
      "ðŸŽ‰ Training pipeline completed successfully!\n",
      "Models are ready for integration with QoS pipeline.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sdn_dataset_2022.csv\", sep=';', comment='@', engine='python')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Drop missing values (only 3 rows)\n",
    "df = df.dropna()\n",
    "\n",
    "# Clean and label 'category' column\n",
    "df['category'] = df['category'].str.strip().str.upper()\n",
    "\n",
    "# Map traffic_class\n",
    "def label_traffic(label):\n",
    "    if isinstance(label, str) and label.startswith('STR-'):\n",
    "        return 'Streaming'\n",
    "    else:\n",
    "        return 'Non-Streaming'\n",
    "\n",
    "df['traffic_class'] = df['category'].apply(label_traffic)\n",
    "\n",
    "# Confirm class distribution\n",
    "print(\"âœ… Data cleaned.\")\n",
    "print(df['traffic_class'].value_counts())\n",
    "print(df[['category', 'traffic_class']].head())\n",
    "\n",
    "# 1. Drop non-feature columns\n",
    "X = df.drop(columns=['category', 'traffic_class'])\n",
    "\n",
    "# Store feature columns for QoS pipeline compatibility\n",
    "feature_columns = X.columns.tolist()\n",
    "print(f\"\\nâœ… Feature columns extracted: {len(feature_columns)} features\")\n",
    "print(f\"Features: {feature_columns[:5]}...\")  # Show first 5 features\n",
    "\n",
    "# 2. Encode the target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['traffic_class'])  # 0 = Non-Streaming, 1 = Streaming\n",
    "\n",
    "print(f\"\\nâœ… Label encoding completed.\")\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Encoded values: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "\n",
    "# 3. Balance with SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "print(f\"\\nâœ… SMOTE balancing completed.\")\n",
    "print(f\"Original class distribution: {pd.Series(y).value_counts().sort_index().values}\")\n",
    "print(f\"Balanced class distribution: {pd.Series(y_res).value_counts().sort_index().values}\")\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Train/test split completed.\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# 5. Train model\n",
    "print(f\"\\nðŸš€ Training Random Forest model...\")\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# 7. Save models and feature schema for QoS pipeline\n",
    "print(f\"\\nðŸ’¾ Saving models for QoS pipeline integration...\")\n",
    "\n",
    "# Save model and label encoder\n",
    "joblib.dump(clf, \"traffic_classifier.pkl\")\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "joblib.dump(feature_columns, \"feature_columns.pkl\")\n",
    "\n",
    "print(\"âœ… Models saved successfully:\")\n",
    "print(\"  - traffic_classifier.pkl\")\n",
    "print(\"  - label_encoder.pkl\") \n",
    "print(\"  - feature_columns.pkl\")\n",
    "\n",
    "# 8. Display model information for verification\n",
    "print(f\"\\nðŸ“Š Model Summary:\")\n",
    "print(f\"Model type: {type(clf).__name__}\")\n",
    "print(f\"Number of estimators: {clf.n_estimators}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Number of classes: {len(le.classes_)}\")\n",
    "print(f\"Classes: {list(le.classes_)}\")\n",
    "\n",
    "# 9. Feature importance (top 10)\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training pipeline completed successfully!\")\n",
    "print(f\"Models are ready for integration with QoS pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5a554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
